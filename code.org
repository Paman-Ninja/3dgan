* Data
** Imports
#+begin_src python :tangle data.py
import h5py, sys
#+end_src
** Datasets
#+begin_src python :tangle data.py
class Dataset:
    def __init__(self, dataset):
        self.dataset = dataset[()]
        # self.dataset = cv.cvtColor(self.dataset, cv.COLOR_BGR2GRAY)
        self.current_pos = 0
        self.num_examples = len(dataset)

    def next_batch(self, batch_size):
        if self.current_pos + batch_size > self.num_examples:
            self.current_pos = 0
        x = self.dataset[self.current_pos : self.current_pos+batch_size]
        self.current_pos += batch_size
        # return (x, None)
        return (x, None)
        # return (np.reshape(x, (batch_size, 64*64*3)), None)

    @property
    def images(self):
        return self.dataset

    @property
    def labels(self):
        return None


class Floorplans:
    def __init__(self, root_dir='/mnt/research/datasets/floorplans/'):
        # self.test = Dataset(os.path.join(root_dir, 'test_set.txt'))
        # self.train = Dataset(os.path.join(root_dir, 'train_set.txt'))
        # self.validation = Dataset(os.path.join(root_dir, 'validation_set.txt'))
        self.file = h5py.File("/mnt/research/projects/hem/datasets/floorplan_64_float32.hdf5", 'r')
        # sys.stdout.write("Loading test...")
        # sys.stdout.flush()
        self.test = Dataset(self.file['test/images'])
        # sys.stdout.write("done!\n\rLoading train...")
        # sys.stdout.flush()
        self.train = Dataset(self.file['train/images'])
        # sys.stdout.write("done!\n\rLoading validation...")
        # sys.stdout.flush()
        self.validation = Dataset(self.file['validation/images'])
        # sys.stdout.write("done!\n\r")
        # sys.stdout.flush()

        # print(self.train, self.test, self.validation)

#+end_src

* Models
** Imports
#+begin_src python :tangle models.py
import tensorflow as tf
#+end_src  
** Fully Connected
#+begin_src python :tangle models.py
def _fc_layer(x, x_size, y_size):
    W = tf.Variable(tf.random_normal([x_size, y_size]))
    b = tf.Variable(tf.random_normal([y_size]))
    l = tf.nn.sigmoid(tf.add(tf.matmul(x, W), b))
    return l


def simple_fc(x, layer_sizes):
    orig_shape = list(x.get_shape())

    with tf.variable_scope('input'):
        # flatten        
        x = tf.contrib.layers.flatten(x)
        flattened_size = int(list(x.get_shape())[1])
        print('input layer:', x, x.get_shape())
        tf.summary.histogram('Input', x)
        
    with tf.variable_scope('encoder'):
        # encoder
        for size in layer_sizes:
            s = int(list(x.get_shape())[1])
            x = _fc_layer(x, s, size)
            tf.summary.histogram('Encoder {}'.format(size), x)
            print('encoder:', x, x.get_shape())

    with tf.variable_scope('decoder'):
        # decoder
        # layer_sizes = list(reversed(layer_sizes[1:]))
        # print('layer_sizes:', layer_sizes, layer_sizes[1::-1][1:])
        for size in layer_sizes[1::-1][1:]:
            s = int(list(x.get_shape())[1])
            x = _fc_layer(x, s, size)
            tf.summary.histogram('Decoder {}'.format(size), x)
            print('decoder:', x, x.get_shape())

        x = _fc_layer(x, int(list(x.get_shape())[1]), flattened_size)
        tf.summary.histogram('Output', x)
        print('final layer:', x, x.get_shape())

    with tf.variable_scope('output'):
        # unflatten
        l = list(orig_shape)[1:]
        l = [-1, int(l[0]), int(l[1]), int(l[2])]
        print('reshape:', l)
        x = tf.reshape(x, l)

    return x
#+end_src
** Convolutional
#+begin_src python :tangle models.py
def _cnn_layer(x, x_size, y_size):
    # 64x64x1 > 64x64x64 > 32x32x128 > 16x16x256 < 32x32x128 < 64x64x64 < 64x64x1
    K = tf.Variable(tf.truncated_normal([3, 3, x_size, y_size], stddev=0.1))
    l = tf.nn.conv2d(x, K, strides=[1, 1, 1, 1], padding='SAME')
    # l = tf.nn.max_pool(l, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')
    return l

def simple_cnn(x, layer_sizes):
    orig_shape = list(x.get_shape())

    print('i)', x.get_shape())

    # encoder
    with tf.variable_scope('encoder'):
        for size in layer_sizes:
            s = int(list(x.get_shape())[3])
            K = tf.Variable(tf.truncated_normal([3, 3, s, size], stddev=0.1))
            x = tf.nn.conv2d(x, K, strides=[1, 1, 1, 1], padding='SAME')
            x = tf.nn.max_pool(x, [1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')
            x = tf.nn.relu(x)
            print('e)', x.get_shape())
            # x = _cnn_layer(x, s, size)

    # decoder
    with tf.variable_scope('decoder'):
        layer_sizes = reversed(layer_sizes)
        for size in layer_sizes:
            s = int(list(x.get_shape())[3])
            K = tf.Variable(tf.truncated_normal([3, 3, s, size], stddev=0.1))
            os = [-1] + list(x.get_shape())[1:]
            os = [os[0], int(os[1])*2, int(os[2])*2, size]
            # print('GET SHAPE:', os)
            x = tf.nn.conv2d_transpose(x, K, output_shape=os, strides=[1, 1, 1, 1], padding='SAME')
            x = tf.nn.relu(x)
            # x = _cnn_layer(x, s, size)
            print('d)', x.get_shape())

        s = int(list(x.get_shape())[3])
        size = int(orig_shape[3])
        K = tf.Variable(tf.truncated_normal([3, 3, s, size], stddev=0.1))

        os = [-1] + list(x.get_shape())[1:]
        os = [os[0], int(os[1])*2, int(os[2])*2, int(os[3])]
        # print('GET SHAPE:', os)
        x = tf.nn.conv2d_transpose(x, K, output_shape=os, strides=[1, 1, 1, 1], padding='SAME')
        x = tf.nn.relu(x)
        print('o)', x.get_shape())
        # x = tf.nn.conv2d(x, K, strides=[1, 1, 1, 1], padding='SAME')
            
            
        # x = _cnn_layer(x, i, i)
        
    return x
#+end_src

* Utility
** Imports
#+begin_src python :tangle util.py
import numpy as np
import tensorflow as tf
import os
import sys
import cv2
import shutil
from data import Floorplans

#+end_src
** Logging
#+begin_src python :tangle util.py
# helper functions
def generate_example_row(data, tensor, xs, include_actual, sess, x_input, args):
    examples = sess.run(tensor, feed_dict={x_input: xs})
    montage = None
    for i, pred in enumerate(examples):

        if include_actual:
            if args.grayscale:
                input_img = cv2.cvtColor(data.test.images[i], cv2.COLOR_BGR2GRAY)
                pred = np.squeeze(pred)
            else:
                input_img = data.test.images[i]
            if args.dataset == 'mnist':
                input_img = np.reshape(input_img, [28, 28, 1])
            # print('pred:', pred.shape, 'input_img:', input_img.shape)
            v = np.vstack((input_img * 255.0, pred * 255.0))
        else:
            if args.grayscale:
                pred = np.squeeze(pred)
            v = pred * 255.0
        montage = v if montage is None else np.hstack((montage, v))
    return montage


def print_progress(epoch, completed, total, loss):
    sys.stdout.write('\r')
    sys.stdout.write('Epoch {:03d}: {:05d}/{:05d}: {:.4f}'.format(epoch, completed, total, loss))
    sys.stdout.flush()


def get_dataset(name):
    print('Loading dataset...')
    if name == 'mnist':
        from tensorflow.examples.tutorials.mnist import input_data
        return input_data.read_data_sets("data/MNIST_data", one_hot=True)
    elif name == 'floorplan':
        return Floorplans()


def prep_workspace(dirname, fresh):
    subdirs = [os.path.join(dirname, "checkpoints"),
               os.path.join(dirname, "images"),
               os.path.join(dirname, "logs")]

    # # yikes... this is probably not a good idea
    # if fresh and os.path.exists(dirname):
    #     shutil.rmtree(dirname)
    #     # os.rmtree(dirname)
        
    if not os.path.exists(dirname):
        os.makedirs(dirname)
    for d in subdirs:
        if not os.path.exists(d):
            os.mkdir(d)
            
    return {'train_loss': open(os.path.join(dirname, "logs", "train_loss.csv"), 'a'),
            'validate_loss': open(os.path.join(dirname, "logs", "validate_loss.csv"), 'a'),
            'test_loss' : open(os.path.join(dirname, "logs", "test_loss.csv"), 'a')}


def plot_loss(image_dir):
    pass


def visualize_parameters():
    total_params = 0
    for variable in tf.trainable_variables():
        shape = variable.get_shape()
        num_params = 1
        for dim in shape:
            num_params *= dim.value
        total_params += num_params
        print('Variable name: {}, size: {}, shape: {}'.format(variable.name, num_params, variable.get_shape()))
    return total_params
#+end_src

* Main
** Imports
#+begin_src python :tangle main.py
import tensorflow as tf, numpy as np, matplotlib.pyplot as plt
import sys, random, argparse, os, uuid, pickle, h5py, cv2, time
# from models import test
from models import simple_fc, simple_cnn
from msssim import MultiScaleSSIM, tf_ssim, tf_ms_ssim
from data import Floorplans
from util import *
#+end_src

** Args
#+begin_src python :tangle main.py
parser = argparse.ArgumentParser()
parser.add_argument('--epochs', type=int, default=3)
parser.add_argument('--batchsize', type=int, default=256)
parser.add_argument('--examples', type=int, default=10)
parser.add_argument('--lr', type=float, default=0.01)
parser.add_argument('--layers', type=int, nargs='+', default=(512, 256, 128))
parser.add_argument('--seed', type=int, default=os.urandom(4))
parser.add_argument('--dataset', type=str, default='mnist')
parser.add_argument('--dir', type=str, default='workspace/{}'.format(uuid.uuid4()))
parser.add_argument('--resume', default=False, action='store_true')
parser.add_argument('--interactive', default=False, action='store_true')
parser.add_argument('--model', type=str, default='fc')
parser.add_argument('--grayscale', default=False, action='store_true')
parser.add_argument('--loss', type=str, default='l1')
parser.add_argument('--optimizer', type=str, default='RMSProp')
parser.add_argument('--momentum', type=float, default=0.01)
parser.add_argument('--fresh', default=False, action='store_true')
parser.add_argument('--decay', type=float, default=0.9)
parser.add_argument('--centered', default=False, action='store_true')
args = parser.parse_args()
#+end_src
** Setup
#+begin_src python :tangle main.py
# for repeatability purposes
random.seed(args.seed)



sess = tf.Session()

# dataset
# TODO: load this last? need the dataset size info though for model creation
data = get_dataset(args.dataset)

# print('SAMPLE IMAGE', data.train.images[0].shape)
# sample_image = data.train.dataset[:1]

if args.dataset == 'mnist':
    x_input = tf.placeholder("float", [None, 784])
    x = tf.reshape(x_input, [-1, 28, 28, 1])
elif args.dataset == 'floorplan':
    x_input = tf.placeholder("float", [None, 64, 64, 3])
    if args.grayscale:
        x = tf.image.rgb_to_grayscale(x_input)
    else:
        x = x_input


# model    
if args.model == 'fc':
    y_hat = simple_fc(x, args.layers)
elif args.model == 'cnn':
    y_hat = simple_cnn(x, args.layers)


    
# loss
if args.loss == 'l1':
    loss = tf.reduce_mean(tf.abs(x - y_hat))
elif args.loss == 'l2':
    loss = tf.reduce_mean(tf.pow(x - y_hat, 2))
elif args.loss == 'rmse':
    loss = tf.sqrt(tf.reduce_mean(tf.pow(x - y_hat, 2)))
elif args.loss == 'ssim':
    loss = 1.0 - tf_ssim(tf.image.rgb_to_grayscale(x), tf.image.rgb_to_grayscale(y_hat))
elif args.loss == 'crossentropy':
    loss = -tf.reduce_sum(x * tf.log(y_hat))
    

# optimizer
if args.optimizer == 'RMSProp':
    optimizer = tf.train.RMSPropOptimizer(args.lr, args.decay, args.momentum, centered=args.centered)
elif args.optimizer == 'Adadelta':
    optimizer = tf.train.AdadeltaOptimizer(args.lr)
elif args.optimizer == 'GD':
    optimizer = tf.train.GradientDescentOptimizer(args.lr)
elif args.optimizer == 'Adagrad':
    optimizer = tf.train.AdagradOptimizer(args.lr)
elif args.optimizer == 'Momentum':
    optimizer = tf.train.MomentumOptimizer(args.lr, args.momentum)
elif args.optimizer == 'Adam':
    optimizer = tf.train.AdamOptimizer(args.lr)
elif args.optimizer == 'Ftrl':
    optimizer = tf.train.FtrlOptimizer(args.lr)
elif args.optimizer == 'PGD':
    optimizer = tf.train.ProximalGradientDescentOptimizer(args.lr)
elif args.optimizer == 'PAdagrad':
    optimizer = tf.train.ProximalAdagradOptimizer(args.lr)

optimizer = optimizer.minimize(loss)
# elif args.optimizer == 'ADAM'

    
global_step = tf.Variable(0, name='global_step', trainable=False)
global_epoch = tf.Variable(1, name='global_epoch', trainable=False)

saver = tf.train.Saver()
sess.run(tf.global_variables_initializer())

    
montage = None

if args.resume:
    #saver = tf.train.import_meta_graph(os.path.join(args.dir, 'model'))
    saver.restore(sess, tf.train.latest_checkpoint(os.path.join(args.dir, 'checkpoints')))
    print('Model restored. Global step:', sess.run(global_step))
        
# workspace
log_files = prep_workspace(args.dir, args.fresh)
if not args.resume:
    pickle.dump(args, open(os.path.join(args.dir, 'settings'), 'wb'))
    tf.train.export_meta_graph(os.path.join(args.dir, 'model'))


# tensorboard
tb_writer = tf.summary.FileWriter(os.path.join(args.dir, 'logs'), graph=tf.get_default_graph())
summary_node = tf.summary.merge_all()



#+end_src
** Training
#+begin_src python :tangle main.py
graph = tf.get_default_graph()
graph.finalize()

total_params = visualize_parameters()
print('Total params: {}'.format(total_params))


start_epoch = sess.run(global_epoch)
for epoch in range(start_epoch, args.epochs+start_epoch):
    epoch_start_time = time.time()
    start_time = time.time()
    # perform training
    n_trbatches = int(data.train.num_examples/args.batchsize)
    completed = 0
    total_train_loss = 0.0
    for i in range(n_trbatches):
        xs, ys = data.train.next_batch(args.batchsize)
        # if args.grayscale:
        #     xs = tf.image.rgb_to_grayscale(xs)
        # if args.grayscale:
        #     xs = cv2.cvtColor(xs, cv2.COLOR_BGR2GRAY)
        _, l = sess.run([optimizer, loss], feed_dict={x_input: xs})
        total_train_loss += l
        completed += args.batchsize
        # sess.run(global_step.assign(completed + (epoch-1)*(n_trbatches*args.batchsize)))
        log_files['train_loss'].write('{:05d},{:.5f}\n'.format(completed + (epoch-1)*(n_trbatches*args.batchsize), l))
        if args.interactive:
            print_progress(epoch, completed, data.train.num_examples, l)
    end_time = time.time()
    if not args.interactive:
        print('Epoch {}: Train loss ({:.5f}), elapsed time {}'.format(epoch, total_train_loss/n_trbatches, end_time-start_time))

    start_time = time.time()
    # perform validation
    n_valbatches = int(data.validation.num_examples/args.batchsize)
    vl = 0.0
    for i in range(n_valbatches):
        xs, ys = data.validation.next_batch(args.batchsize)
        vl += sess.run(loss, feed_dict={x_input: xs})
    end_time = time.time()
    log_files['validate_loss'].write('{:05d},{:.5f}\n'.format(completed + (epoch-1)*(n_trbatches*args.batchsize), vl/n_valbatches))
    if args.interactive:
        sys.stdout.write(', validation: {:.4f}'.format(vl/n_valbatches))
        sys.stdout.write('\r\n')
    else:
        print('Epoch {}: Validation loss ({:.5f}), elapsed time {}'.format(epoch, vl/n_valbatches, end_time - start_time))

    # montage
    if args.interactive:
        sys.stdout.write('Generating examples to disk...')
    else:
        print('Generating examples to disk...')
    # TODO: should reshape this on the fly, and only if necessary
    examples = data.test.images[:args.examples]
    # if args.dataset == 'mnist':
    #     examples = data.test.images[:args.examples]
    # else:
    #     examples = data.test.dataset[:args.examples]
    # tf.reshape(tf.image.rgb_to_grayscale(x), (-1, 64*64))
        
    # examples = np.reshape(examples, (args.examples, 64*64*3))
    # examples = np.reshape(examples, (args.examples, 64*64))
    # examples = tf.image
    row = generate_example_row(data, y_hat, examples, epoch==1, sess, x_input, args)
    if montage is not None:
        print('row:', row.shape, 'montage:', montage.shape)
    imgfile = os.path.join(args.dir, 'images', 'montage_{:03d}.png'.format(epoch))
    cv2.imwrite(imgfile, row)
    montage = row if montage is None else np.vstack((montage, row))
    if args.interactive:
        sys.stdout.write('complete!\r\n')
        sys.stdout.flush()

    # sess.run(global_epoch.assign(epoch+1))

    # tensorboard
    summary_result = sess.run(summary_node, feed_dict={x_input: xs})
    tb_writer.add_summary(summary_result, epoch)
        
    # snapshot
    if args.interactive:
        sys.stdout.write('Writing snapshot to disk...')
    else:
        print('Writing snapshot to disk...')
    chkfile = os.path.join(args.dir, 'checkpoints', 'epoch_{:03d}.ckpt'.format(epoch))
    saver.save(sess, chkfile, global_step=global_step)
    if args.interactive:
        sys.stdout.write('complete!\r\n')
        sys.stdout.flush()
    epoch_end_time = time.time()
    print('Total elapsed epoch time: {}'.format(epoch_end_time - epoch_start_time))

#+end_src
** Testing
#+begin_src python :tangle main.py
# save complete montage
cv2.imwrite(os.path.join(args.dir, 'images', 'montage.png'), montage)
    
# perform test
n_tebatches = int(data.test.num_examples/args.batchsize)
tel = 0.0
completed = 0
for i in range(n_tebatches):
    xs, ys = data.test.next_batch(args.batchsize)
    tel += sess.run(loss, feed_dict={x_input: xs})
    completed += args.batchsize
    if args.interactive:
        sys.stdout.write('\r')
        sys.stdout.write('test: {:.4f}'.format(l))
        sys.stdout.flush()
log_files['test_loss'].write('{:05d},{:.5f}\n'.format((epoch) * n_trbatches * args.batchsize, tel/n_tebatches))
if args.interactive:
    sys.stdout.write('\r\n')
else:
    print('Test loss: {:.5f}'.format(tel/n_tebatches))

# close down log files
for key in log_files:
    log_files[key].close()

# generate charts
train_loss = np.genfromtxt(os.path.join(args.dir, "logs", "train_loss.csv"), delimiter=',')
test_loss = np.genfromtxt(os.path.join(args.dir, "logs", "test_loss.csv"), delimiter=',')
validate_loss = np.genfromtxt(os.path.join(args.dir, "logs", "validate_loss.csv"), delimiter=',')
plt.rc('text', usetex=True)
plt.rc('font', **{'family':'serif','serif':['Palatino']})
for x in [(train_loss, {}), (validate_loss, {'color': 'firebrick'})]:
    data, plot_args = x
    iters = data[:,[0]]
    vals = data[:,[1]]
    plt.plot(iters, vals, **plot_args)
    plt.xlabel('Iteration')
    plt.ylabel(r'$\ell_1$ Loss')
plt.savefig(os.path.join(args.dir, "images", "loss.pdf"))
#+end_src
